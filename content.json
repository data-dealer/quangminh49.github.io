{"meta":{"title":"Minh's Blog","subtitle":"","description":"","author":"Minh Nguyen","url":"https://quangminh49.github.io","root":"/"},"pages":[{"title":"About me","date":"2021-04-27T03:44:37.000Z","updated":"2021-04-27T09:00:16.878Z","comments":true,"path":"about.html","permalink":"https://quangminh49.github.io/about.html","excerpt":"","text":"----------------- Nguyen Quang Minh 0914198598 &#x6e;&#103;&#117;&#121;&#101;&#110;&#113;&#117;&#97;&#x6e;&#x6d;&#x69;&#110;&#104;&#52;&#57;&#x39;&#54;&#64;&#103;&#x6d;&#97;&#105;&#108;&#46;&#99;&#111;&#x6d; District 2, HCM city EDUCATION : Banking University Of Ho Chi Minh City: Finance and Banking Management information systems SKILLS Machine learning: Proficient in building credit scoring model and implementing in business. Build object detection model with keras Build chatbot with ntlk, rasa Database: SQL: Oracle, MySql - build the data-mart for Machine learning models and reports No-SQL: MongoDB - build database for small company. Python: Build machine learning model with Sklearn, Keras, nltk, … Object-oriented programming. Data analysis with pandas, numpy Data visualization with matplotlib, seaborn, plotly Crawl data and build automation tool with selenium, beautiful soup, requset, request-html,… Build website, restful-Api with Flask, Django. JavaScript: Build many small web applications with google ecosystem. Web Development with React. Mobile App Development with React Native. BI: Data-studio, Python-Dash WORK EXPERIENCE: Senior Data Modeling at VPBank Finance Company Limited - FECREDIT (07/2019 - Present) Building Credit Scorecard models for Loan - B1, B2, B3 and Card-Product. Building automation reports on development and operation of Credit Scorecard model. Use machine learning and statistics on optimizing and forecasting business operations. Deployment specialist at SS4U Softwares Join Stock Company (04/2018 - 10/2018) Business analysis: Manufacturing, purchasing, selling, inventory and accounting. Building administrative reports. HONORS &amp; AWARDS: Best Performance 2019 of FeCredit Entering the semi-finals of the Entropy Data Analytics Competition organized by the John Von Neumann Institute."}],"posts":[{"title":"CI/CD chapter 1 - Jenkins/Github/EC2","slug":"CI-CD with jenkins part 1","date":"2021-02-15T17:00:00.000Z","updated":"2021-04-27T08:19:44.850Z","comments":true,"path":"2021/02/16/CI-CD with jenkins part 1/","link":"","permalink":"https://quangminh49.github.io/2021/02/16/CI-CD%20with%20jenkins%20part%201/","excerpt":"Bài này mình thực hành trên con EC2 của AWS, nên 1 tài khoản AWS là điều kiện cần, khi tạo tài khoản mới thì chúng ta sẽ được miễn phí 1 năm rất nhiều dịch vụ của AWS, thực hành phủ phê luôn.","text":"Bài này mình thực hành trên con EC2 của AWS, nên 1 tài khoản AWS là điều kiện cần, khi tạo tài khoản mới thì chúng ta sẽ được miễn phí 1 năm rất nhiều dịch vụ của AWS, thực hành phủ phê luôn. Cấu hình Jenkins ServerStep 1: Tạo Instance EC2 cho Jenkins ServerChọn Amazon Linux 2 AMI với các tuỳ chọn cơ bản cho server thực hành. Tạo Thư mục chứa key.pem và 1 file bash script để ssh vào server cho tiện 12├── key.pem└── ssh_server.sh ssh_server.sh: 1ssh -i &quot;key.pem&quot; [username]@[IPv4 address] và giờ để remote vào Server Jenkins thì chúng ta chỉ cần: 1sh ssh_server.sh Step 2: Cài Javacần quyền root để tiếp tục cài đặt 123sudo suyum install -y java-1.8.0-openjdk-devel.x86_64alternatives --config java check java version 123456789[root@ip-172-31-xx-xxx user]# java -versionopenjdk version &quot;1.8.0_282````## Step 3: Cài Jenkins```shwget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat/jenkins.reporpm --import https://pkg.jenkins.io/redhat/jenkins.io.keyyum install -y jenkins Step 3: Start Jenkins12systemctl start jenkinstelnet localhost 8080 Jenkins mặc định mở tại cổng 8080 nên có thể dùng telnet để check xem có Start thành công không lấy mật khẩu Admin Jenkins: 1cat /var/lib/jenkins/secrets/initialAdminPassword Với EC2 thì cần mở thêm port 8080 trong Security Groups thì mới có thể truy 8080 từ publish Ip. Bây giờ chúng ta có thể truy cập vào Jenkins vừa khởi tạo tại [IPv4 address]:8080 Nhập mật khẩu Admin Jenkins vào đây và cài đặt theo các Install Suggeted Plugins là được. Đến bước Instance Configuration thì có thể điền theo cấu trúc http://[IPv4 address]:8080/ hoặc domain đã được cấu hình DNS Đăng nhập với đường link ta vừa config được như vầy coi như đã setup thành công ! Cấu hình CI/CDTiếp theo chúng ta sẽ sử dụng Jenkin để listen nhữn thay đổi từ Github Repository và depoy lên web server. Mình sẽ dùng Apache Để dựng web server cho bài này nên chúng ta cần cài dặt thêm cả Git và Apache cho con EC2. Step 1: Cài đặt các gói cần thiết:1234yum update -yyum install httpd -yyum install git -yamazon-linux-extras install epel 1systemctl start httpd Step 2: Tạo Github Repository:12cd /opt/demo-web-cicdecho &quot;Hello world&quot; &gt; index.html Đẩy Repository lên github: Cấu hình webhook Vào phần Setting&gt;Webhook, cấu hình Webhook (http://[IP]:[JENKINS-PORT]/github-webhook) để Github gửi các action tới Jenkens server. Step 3: Tạo Cấu hình Apache:1vim /etc/httpd/conf/httpd.conf 1234567891011Listen 81NameVirtualHost *:81&lt;VirtualHost *:81&gt; DocumentRoot &#x2F;opt&#x2F;demo-web-cicd&#x2F; &lt;Directory &quot;&#x2F;opt&#x2F;demo-web-cicd&quot;&gt; Order deny,allow Allow from all AllowOverride All Require all granted &lt;&#x2F;Directory&gt;&lt;&#x2F;VirtualHost&gt; 1systemctl restart httpd Vào Web Server đang chạy ở port 81 : Step 4: Cấu hình JenkinsPhân quyền cho user jenkins vào thư mục để pull code 1chown -Rf jenkins:root &#x2F;opt&#x2F;demo-web-cicd&#x2F; Vào Jenkins server Chọn New Item Thêm Git Repository và Credentials Chọn trigger là Github hook trigger Chọn Build Execute shell Push code lên Github Repository Sau khi cấu hình xong Chọn Build Now để chạy thử, có tick xanh như vậy là build thành công rồi đó. Thực hiện thay đổi nội dung index.html thành “Hello girls” và ten tén ten …","categories":[],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://quangminh49.github.io/tags/DevOps/"},{"name":"CI/CD","slug":"CI-CD","permalink":"https://quangminh49.github.io/tags/CI-CD/"}]},{"title":"CI/CD chapter 0 - Intro","slug":"CI-CD","date":"2021-02-14T03:17:55.000Z","updated":"2021-04-27T08:13:58.988Z","comments":true,"path":"2021/02/14/CI-CD/","link":"","permalink":"https://quangminh49.github.io/2021/02/14/CI-CD/","excerpt":"Ngày nay khi triết lý Agile được áp dụng ở mọi nơi. Cũng chính vì Tính nhanh và linh hoạt của Agile mà vai trò của quy trình CI/CD (Continuous Integration/ Continuous Delivery) lại càng trở lên cấp thiêt hơn bao giờ hết. Sau một thời gian mình triển khai các service từ Web app, Rest Api cho đến Database bằng cơm, và khi các service càng nhiều và phức tạp thì việc triển khai service bằng cơm trỏ lên khá mệt mỏi và thiêú chính xác. Do vậy mình đã quyết tâm áp dụng các giải pháp CI/CD vào các Project hiện tại, cũng là để chuẩn bị cho một tầm nhìn dài hạn hơn.","text":"Ngày nay khi triết lý Agile được áp dụng ở mọi nơi. Cũng chính vì Tính nhanh và linh hoạt của Agile mà vai trò của quy trình CI/CD (Continuous Integration/ Continuous Delivery) lại càng trở lên cấp thiêt hơn bao giờ hết. Sau một thời gian mình triển khai các service từ Web app, Rest Api cho đến Database bằng cơm, và khi các service càng nhiều và phức tạp thì việc triển khai service bằng cơm trỏ lên khá mệt mỏi và thiêú chính xác. Do vậy mình đã quyết tâm áp dụng các giải pháp CI/CD vào các Project hiện tại, cũng là để chuẩn bị cho một tầm nhìn dài hạn hơn. Thực sự thì CI/CD khá dễ hiểu, nó gắn liền các sản phẩm công nghệ và vận hành của doanh nghiệp, hình bên dưới đây sẽ minh hoạ rất rõ điều đó : Ngày nay có rất nhiều công cụ hỗ trợ chúng ta trong quá trình CI/CD, mỗi công cụ đều có ưu và nhược điểm riêng, và để có thể ứng dụng chúng thì mỗi DevOps phải chuẩn bị cho mình môt nền tảng kiến thức vững chắc để có thể thực sự đáp ứng cả quy trình CI/CD. Trong series này, mình sẽ cố viết các bài với Jenkins và GitLab 9, các tool khác thì khi chúng ta nắm vững lý thuyết thì cũng dễ dàng triển khai thôi.","categories":[],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://quangminh49.github.io/tags/DevOps/"},{"name":"CI/CD","slug":"CI-CD","permalink":"https://quangminh49.github.io/tags/CI-CD/"}]},{"title":"CRM SYSTEM","slug":"CRM system","date":"2020-12-07T03:17:55.000Z","updated":"2021-04-27T08:13:32.404Z","comments":true,"path":"2020/12/07/CRM system/","link":"","permalink":"https://quangminh49.github.io/2020/12/07/CRM%20system/","excerpt":"Dẫn nhậpChuyện là mình có một Job remote ở 1 công ty nọ từ giờ tạm thời gọi là cty G cho tiện. Công ty hoạt động trong lĩnh vực dịch vụ cũng được gần 10 năm nay. Tình hình cty thì vấn khá ổn, nhưng để bắt kịp với sự thay đổi chóng mặt của thời đại công ty có cộng tác với team mình. Vai trò của mình tại đây thì rộng lắm, data, dev… nói túm lại là làm giải pháp công nghệ cho công ty (nghe vĩ mô chưa !!!). Tính đến thời điểm này mình cũng đã làm được hơn 6 tháng cho cty G rồi, cũng đã có dự án thành công, nên tiện đây kể 1 câu chuyên về dự án đầu tay tại cty G.","text":"Dẫn nhậpChuyện là mình có một Job remote ở 1 công ty nọ từ giờ tạm thời gọi là cty G cho tiện. Công ty hoạt động trong lĩnh vực dịch vụ cũng được gần 10 năm nay. Tình hình cty thì vấn khá ổn, nhưng để bắt kịp với sự thay đổi chóng mặt của thời đại công ty có cộng tác với team mình. Vai trò của mình tại đây thì rộng lắm, data, dev… nói túm lại là làm giải pháp công nghệ cho công ty (nghe vĩ mô chưa !!!). Tính đến thời điểm này mình cũng đã làm được hơn 6 tháng cho cty G rồi, cũng đã có dự án thành công, nên tiện đây kể 1 câu chuyên về dự án đầu tay tại cty G. Câu chuyện dữ liệu","categories":[],"tags":[{"name":"projects","slug":"projects","permalink":"https://quangminh49.github.io/tags/projects/"}]},{"title":"Field Application chapter 0 - Intro","slug":"Field Application chapter 0","date":"2020-12-06T17:00:00.000Z","updated":"2021-04-28T11:05:02.750Z","comments":true,"path":"2020/12/07/Field Application chapter 0/","link":"","permalink":"https://quangminh49.github.io/2020/12/07/Field%20Application%20chapter%200/","excerpt":"Năm vừa qua thì mình đã tham gia một dự án khá dài hơi trong đó vai trò của mình khá rộng, vừa tham gia thiết kế giải pháp cho đến thiết kế kiến trúc rồi cả Full Stack Dev trong team luôn. Cũng nhờ vậy mà có chút khinh nghiệm để viết series Field Application này. Series Field Application sẽ mô tả quá trình mà một Dự án phần mềm từ lúc bắt đâù cho đến khi đi vào vận hành như thế nào. Những kiến thức trình bày trong series hoàn toàn là tự mình mày mò tìm hiếu và áp dụng thành công trong dự án. Các nội dung chính: Phân tích thiết kế hệ thống Quy trình phát triển Lập trình: Backend: Django Mobile: React Native Bài mở đầu mình sẽ giới thiệu về hoàn cảnh, các giải pháp, cũng như triết lý phát triển dự án. Lưu ý: Tuy các kiến thức là thật nhưng các hoàn cảnh, nhân tố, data, cty G .. trong series là hoàn toàn hư cấu và mang tính minh hoạ","text":"Năm vừa qua thì mình đã tham gia một dự án khá dài hơi trong đó vai trò của mình khá rộng, vừa tham gia thiết kế giải pháp cho đến thiết kế kiến trúc rồi cả Full Stack Dev trong team luôn. Cũng nhờ vậy mà có chút khinh nghiệm để viết series Field Application này. Series Field Application sẽ mô tả quá trình mà một Dự án phần mềm từ lúc bắt đâù cho đến khi đi vào vận hành như thế nào. Những kiến thức trình bày trong series hoàn toàn là tự mình mày mò tìm hiếu và áp dụng thành công trong dự án. Các nội dung chính: Phân tích thiết kế hệ thống Quy trình phát triển Lập trình: Backend: Django Mobile: React Native Bài mở đầu mình sẽ giới thiệu về hoàn cảnh, các giải pháp, cũng như triết lý phát triển dự án. Lưu ý: Tuy các kiến thức là thật nhưng các hoàn cảnh, nhân tố, data, cty G .. trong series là hoàn toàn hư cấu và mang tính minh hoạ Toàn cảnh dự ánCông ty G - một công ty hoạt động trong lĩnh vực phân phối sỉ và lẻ hàng tiêu dùng với hơn 300 nhân viên thị trường. Quy mô và doanh thu của công ty G luôn tăng trưởng trong 3 năm gần đây. Tuy nhiên Tốc độ tăng trưởng doanh thu lại kém hẳn so với tốc độ tăng nhân lực, công ty đã gặp gặp rất nhiều khó khăn khi quy mô mở rộng quá nhanh còn quy trình và công nghệ quản lý thì lại không đáp ứng nổi. Các vấn đề Khó khăn quản lý chất lượng (nhân viên đi làm đối phó, không đi làm). Lộ thông tin khách hàng, mất lợi thế cạnh tranh do dữ liệu được lưu truyền bằng giấy, excel,.. Quản lý không nắm được tình hình danh mục. Truyền thông các chính sách tới nhân viên chậm chạp Do vậy đội dự án của mình được mời làm giải pháp quản lý mảng thị trường cho công G để giải quyết những vấn đề trên. Mô hình hoạt động Nhân viên. Trưởng nhóm. Quản lý UseCase Diagram dươí đây sẽ diễn tả nhiệm vụ của các bên: Giải phápChúng ta sẽ build một Ứng dụng mobile, để phục vụ tất cả các đối tượng trên. Ứng dụng sẽ dựa trên các nền tảng công nghệ dưới đây Mô hình phát triển dự án AgileTeam mình dùng mô hình Agile để phát triển dự án vì nó thực sự hiệu quả và phù hợp với nguồn lực hiện có. bản tuyên ngôn Agile: Cá nhân và sự tương tác hơn là quy trình và công cụ. Phần mềm chạy tốt hơn là tài liệu đầy đủ. Cộng tác với khách hàng hơn là đàm phán hợp đồng. Phản hồi với sự thay đổi hơn là bám theo kế hoạch Ưu điểm: Giảm thời gian cần thiết để tận dụng một số tính năng của hệ thống. Kết quả cuối cùng là phần mềm chất lượng cao trong thời gian ít nhất có thể và sự hài lòng của khách hàng. Nhược điểm: Yêu cầu cao về kỹ năng và ý thức của các thành viên. Tài liệu thường có chậm và ít.","categories":[],"tags":[{"name":"Full-Stack","slug":"Full-Stack","permalink":"https://quangminh49.github.io/tags/Full-Stack/"}]},{"title":"Rasa chatbot intro","slug":"chatbot 1","date":"2020-04-10T17:00:00.000Z","updated":"2021-04-27T14:01:18.021Z","comments":true,"path":"2020/04/11/chatbot 1/","link":"","permalink":"https://quangminh49.github.io/2020/04/11/chatbot%201/","excerpt":"Mở bài thì mình cũng lan man chút về cái thời buổi Covid khó khăn, thời gian qua mình đã nỗ lực rất nhiều nhưng vì cái đại dịch vô duyên này mà mọi chuyện rối tung hết, chẳng đâu vào đâu. Có lúc cũng suy nghĩ rằng hay mình tìm chỗ khác lương cao hơn nhưng mấy đêm suy nghĩ cũng vẫn quyết định ở lại. Vì rằng tiền thì từ từ kiếm, năng lực mình còn đó sớm muộn gì thì cũng sẽ đạt được thôi. Nhớ lại lúc ban đầu khi vào công ty cũng vì cảm thấy có niềm tin ở anh sếp, và rồi cái cảm giác đó đã đúng. Ở đây mình đã được tin tưởng, các dự án mình làm đa số được ứng dụng, nên là thời gian tuy không nhiều nhưng cũng được bào kha khá, ahihi. Nhưng chính nhờ đó mà kinh nghiệm và trình độ của mình đã tăng lên rất nhanh và sẽ còn tăng mạnh trong thời gian tới.","text":"Mở bài thì mình cũng lan man chút về cái thời buổi Covid khó khăn, thời gian qua mình đã nỗ lực rất nhiều nhưng vì cái đại dịch vô duyên này mà mọi chuyện rối tung hết, chẳng đâu vào đâu. Có lúc cũng suy nghĩ rằng hay mình tìm chỗ khác lương cao hơn nhưng mấy đêm suy nghĩ cũng vẫn quyết định ở lại. Vì rằng tiền thì từ từ kiếm, năng lực mình còn đó sớm muộn gì thì cũng sẽ đạt được thôi. Nhớ lại lúc ban đầu khi vào công ty cũng vì cảm thấy có niềm tin ở anh sếp, và rồi cái cảm giác đó đã đúng. Ở đây mình đã được tin tưởng, các dự án mình làm đa số được ứng dụng, nên là thời gian tuy không nhiều nhưng cũng được bào kha khá, ahihi. Nhưng chính nhờ đó mà kinh nghiệm và trình độ của mình đã tăng lên rất nhanh và sẽ còn tăng mạnh trong thời gian tới. Vì khó khăn nên công ty quyết định cắt 25% lương và phụ cấp, chắc đợt này tổng thu nhập mình giảm khoảng 40-50% ,tuy vậy cũng quyết định hết sức cùng công ty sẽ đóng góp hết mình vào các giải pháp tình thế mà công ty đưa ra. Nhưng mà thực sự mình cũng khó lắm, lương không cao mà giảm ngần đó sợ là không trụ nổi 2,3 tháng vừa đúng lúc hết hợp đồng. Coi như kham khổ đánh cuộc cùng công ty đi, nếu qua thời gian đấy mà không khá hơn được chắc mình cũng phải tìm đường khác chứ chắc không đủ sống. Cũng chẳng muốn đâu, nhưng hy vọng nếu sếp đọc được thì cũng hiểu cho em ạ, hiện giờ em sẽ hết mình vì công ty, nhưng đến mức đó thì em cũng sẽ nói anh trước ạ ! Chúng ta không nên đứng núi này trông núi nọ nhưng cũng nên chuẩn bị cho tình huống xấu nhất chứ. Các cụ ta có câu : “Thỏ khôn đào sẵn 3 hang” lại có câu “không sợ nhất vạn, chỉ sợ vạn nhất” mình vẫn tin tưởng rằng dù tình huống nào có xảy ra khi căn cơ mình vững chắc và chuẩn bị đầy đủ thì không có gì phải sợ hãi cả. Đó cũng là động lực để mình làm series Chatbot, vừa là bồi dưỡng căn cơ, vừa tìm thêm một phương án dự phòng. Chatbot là gì ?Chatbot là một công cụ có thể giao tiếp, tương tác với con người thông qua trí tuệ nhân tạo đã được lập trình sẵn. Trong hầu hết các trường hợp thì chatbot được sử dụng qua ứng dụng nhắn tin để nói chuyện với con người. Chatbot có thể sử dụng như một công cụ hiệu quả giúp cho việc chăm sóc khách hàng được thực hiện tự động 24/7. Trên thực tế, chatbot nhanh hơn con người trong việc đưa ra câu trả lời. Hơn nữa, với chatbot các doanh nghiệp sẽ không cần thuê nhân viên để chăm sóc khách hàng. Điều này giúp cho chi phí marketing của doanh nghiệp được giảm xuống. Ai nên sử dụng Chatbot? Chatbot đang dần được ứng dụng sâu rộng vào cuộc sống cũng như kinh doanh. Đối tượng sử dụng Chatbot rất đa dạng từ cho tất cả các ngành kinh doanh nhưng tốt nhất trong các nhóm ngành như: - Kinh doanh thời trang: quần áo, trang sức, giầy dép, phụ kiện… - Làm đẹp: Spa, Thẩm mỹ viện, Làm tóc, nail… - Ẩm thực: Nhà hàng, Quán ăn, Quán cafe… - Giáo dục – đào tạo: trường học, trung tâm đào tạo ngoại ngữ, kỹ năng, dạy nghề, tuyển sinh… - Các dịch vụ hỗ trợ: đặt phòng, đặt vé, vận tải… - Các dịch vụ bán hàng Online trực tuyến: Bán đủ thứ trên đời miễn là có 1 fanpage. Ý tưởng:Trước mắt mình sẽ demo một con chatbot trên Facebook mang ý tưởng kết nối với nhà tuyển dụng, phòng khi tình thế khó khăn. Nó sẽ thực hiện các chức năng trả lời câu hỏi và giới thiệu về bản thân mình. Nền tảng sử dụng: Đầu tiên mình dùng thư viện NLTK, mọi thứ cũng khá dễ dàng để tạo một chatbot nhưng phần kết nối với Facebook code hơi cực. Thế là mình tìm đến Rasa - nền tảng chatbot toàn diện mọi thứ trở nên đơn giản đến bất ngờ, nguồn tài liệu hướng dẫn tiếng Việt cũng rất nhiều nên các bạn có thể tạo chatbot cơ bản cho mình chỉ trong vài phút. Sau đây là một số thành quả của mình : ![Crepe]/img/bot_3.png) Demo đơn giản vậy thôi. Tuy nhiên khi chúng ta ứng dụng chatbot vào trong mô trường kinh doanh thì chúng ta còn rất nhiều vấn đề cần giải quyết. Có cơ hội mình sẽ viết ở kỳ sau","categories":[],"tags":[{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"},{"name":"NLP","slug":"NLP","permalink":"https://quangminh49.github.io/tags/NLP/"}]},{"title":"HCM housing chapter 2 - Visualization","slug":"HCM housing chapter 2","date":"2020-04-04T17:00:00.000Z","updated":"2021-04-27T14:02:30.246Z","comments":true,"path":"2020/04/05/HCM housing chapter 2/","link":"","permalink":"https://quangminh49.github.io/2020/04/05/HCM%20housing%20chapter%202/","excerpt":"Currently, I am crawling real estate data to forecast future house price trends. In the meantime why not make a cool dashboard and we can use it to track daily house price movements, as easy as eating a cake, get started.","text":"Currently, I am crawling real estate data to forecast future house price trends. In the meantime why not make a cool dashboard and we can use it to track daily house price movements, as easy as eating a cake, get started. Step 1: Create a Google spreadsheet file on my Google drive to store the data I’ve collected. Step 2: Create your own Data Studio project: Step 3: Connect the Data studio with the Google spreadsheet we just created: Step 4: Reformat the data fields: Step 5: Drag and drop to enjoy the results !!!","categories":[],"tags":[{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"},{"name":"HCM-housing","slug":"HCM-housing","permalink":"https://quangminh49.github.io/tags/HCM-housing/"}]},{"title":"Staff capacity forecast","slug":"statistic_1","date":"2020-03-21T17:00:00.000Z","updated":"2021-04-28T06:59:44.747Z","comments":true,"path":"2020/03/22/statistic_1/","link":"","permalink":"https://quangminh49.github.io/2020/03/22/statistic_1/","excerpt":"Context","text":"Context","categories":[],"tags":[{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"},{"name":"Statistic","slug":"Statistic","permalink":"https://quangminh49.github.io/tags/Statistic/"}]},{"title":"HCM housing chapter 1 - Price prediction","slug":"HCM housing chapter 1 Linear Regression","date":"2020-03-14T17:00:00.000Z","updated":"2021-04-27T14:02:25.615Z","comments":true,"path":"2020/03/15/HCM housing chapter 1 Linear Regression/","link":"","permalink":"https://quangminh49.github.io/2020/03/15/HCM%20housing%20chapter%201%20Linear%20Regression/","excerpt":"Bài này mình sẽ nói thêm 1 chút về cách xử lý dữ liệu cũng như xây dựng mô hình dự đoán giá nhà cơ bản và rất đơn giản. Ứng dụng: - Sử dụng mô hình để thẩm định một sảm phẩm BDS mới, sản phẩm đang có giá cao hay thấp so với thị trường. Mô hình có tính tham khảo cao với cả nhà đầu tư cá nhân và các tổ chức. Let’ go","text":"Bài này mình sẽ nói thêm 1 chút về cách xử lý dữ liệu cũng như xây dựng mô hình dự đoán giá nhà cơ bản và rất đơn giản. Ứng dụng: - Sử dụng mô hình để thẩm định một sảm phẩm BDS mới, sản phẩm đang có giá cao hay thấp so với thị trường. Mô hình có tính tham khảo cao với cả nhà đầu tư cá nhân và các tổ chức. Let’ go Step 1 : Clean dataPhần này xử lý dữ liệu khá giống trong kỳ trước: Chúng ta đã làm được các việc sau: bỏ 1 số trường không dùng đổi tên trường tách giá trị số từ chuỗi .dataframe tbody tr th:only-of-type { vertical-align: middle; } table { font-size: 12px; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th &#123; text-align: right; &#125; floors district street price area facade road_wide bedrooms toilets furniture 0 2.0 Thủ Đức Linh Xuân 3.50 75.0 4.0 6.5 3.0 2.0 Full nội thất cơ bản 1 2.0 Thủ Đức Linh Xuân 1.87 40.0 4.0 3.0 2.0 2.0 Không. 2 2.0 Thủ Đức Linh Xuân 1.87 40.0 4.0 3.0 2.0 2.0 Cơ bản. 3 2.0 Thủ Đức Linh Xuân 1.20 35.0 5.0 4.0 2.0 2.0 NaN 4 2.0 Thủ Đức Linh Xuân 4.45 140.0 5.0 4.0 9.0 4.0 NaN Xử lý thêm trường furnitureNhưng vẫn còn 1 trường là ‘furniture’ chúng ta chưa xử lý, theo ý kiến cá nhân thì giá BDS cũng sẽ được tác động từ nội thất khá nhiều nên là chúng ta vẫn nên xử lý biến này ! values : 833 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 4600 entries, 0 to 4901 Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 furniture 1693 non-null object dtypes: object(1) memory usage: 71.9+ KB None Cao cấp 56 Cao cấp. 55 Đầy đủ. 53 Nội thất cao cấp. 48 Đầy đủ 47 Name: furniture, dtype: int64 Tuy nhiên có một vấn đề là dữ liệu kiểu nhập tự do nên khá khó phân tích, có đến 833 giá trị nhập khác nhau trên 1693 giá trị không null Nhận thấy rằng đa số các nhà có nột thất thì họ mới ghi vào phần ‘furniture’, do vậy chúng ta có thể tạo 1 trường dữ liệu để thể hiện là nhà có nột thất hay không. Một cách tốt hơn nữa là chúng ta sẽ phân tích cấu trúc chuỗi và phân loại nội thất theo: cao cấp cơ bản không Cách này thì mình nghĩ sẽ giữ được nhiều thông tin hơn nên các bạn có cơ hội thì làm thử nhé, mình làm cơ bản cho nhanh đã 0 1 1 1 2 1 3 0 4 0 Name: furniture, dtype: int64 Xử lý missing valuesMissing values: %null #null bedrooms 0.478261 22 floors 3.086957 142 Với các dữ liệu missing values thì chúng ta có khá nhiều cách xử lý. Nhưng quan trọng nhất vẫn là ta hiểu được tại sao dữ liệu đó lại missing. Trong trường hợp của chúng ta thì dữ liệu missing do yếu tố chủ quan của người đăng tin, quên nhập, không nhập dữ liệu vào. Với dữ liệu numeric có một số cách để xử lý dữ liệu missing như thay bằng 1 giá trị số cụ thể như 0, -99, cũng có thể thay bằng giá trị mode, median , mean của trường dữ liệu, hoặc tạo 1 mô hình để dự đoán chính giá trị missing đó. Vậy chúng ta hãy cùng thử nghiệm 1 số phương pháp trên để tìm ra phương án tốt nhất. Bộ dữ liệu thay thế giá trị missing = 0: .dataframe tbody tr th:only-of-type { vertical-align: middle; } table { font-size: 12px; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th &#123; text-align: right; &#125; floors district street price area facade road_wide bedrooms toilets furniture 0 2.0 Thủ Đức Linh Xuân 3.50 75.0 4.0 6.5 3.0 2.0 1 1 2.0 Thủ Đức Linh Xuân 1.87 40.0 4.0 3.0 2.0 2.0 1 2 2.0 Thủ Đức Linh Xuân 1.87 40.0 4.0 3.0 2.0 2.0 1 Bộ dữ liệu thay thế giá trị missing = median: .dataframe tbody tr th:only-of-type { vertical-align: middle; } table { font-size: 12px; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th &#123; text-align: right; &#125; floors district street price area facade road_wide bedrooms toilets furniture 0 2.0 Thủ Đức Linh Xuân 3.50 75.0 4.0 6.5 3.0 2.0 1 1 2.0 Thủ Đức Linh Xuân 1.87 40.0 4.0 3.0 2.0 2.0 1 2 2.0 Thủ Đức Linh Xuân 1.87 40.0 4.0 3.0 2.0 2.0 1 Step 2 : VisualizationXem phân phối của các biến: correlation matrix cho ta thấy sự tương quan giữa các biến, giá trị cao (max = 1) cho sự đồng biến giá trị thấp (min =-1) cho nghịch biến và bàng quan khi = 0. array([&#39;price&#39;, &#39;area&#39;, &#39;road_wide&#39;, &#39;bedrooms&#39;, &#39;toilets&#39;], dtype=object) Top các thuộc tính có tương quan cao với biến price: Xem qua quan hệ tuyến tính giữa các biến độc lập và biến phụ thuộc: Hồi quy tuyến tính với phương pháp OLS OLS Regression Results Dep. Variable: price R-squared: 0.565 Model: OLS Adj. R-squared: 0.558 Method: Least Squares F-statistic: 88.16 Date: Sun, 05 Apr 2020 Prob (F-statistic): 7.54e-82 Time: 16:03:41 Log-Likelihood: -795.61 No. Observations: 484 AIC: 1607. Df Residuals: 476 BIC: 1641. Df Model: 7 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const -0.9891 0.338 -2.924 0.004 -1.654 -0.324 floors 0.3321 0.095 3.501 0.001 0.146 0.518 area 0.0257 0.002 10.916 0.000 0.021 0.030 facade 0.1948 0.052 3.777 0.000 0.093 0.296 road_wide 0.1615 0.022 7.280 0.000 0.118 0.205 bedrooms -0.0531 0.088 -0.607 0.544 -0.225 0.119 toilets 0.4166 0.077 5.427 0.000 0.266 0.567 furniture 0.1319 0.120 1.098 0.273 -0.104 0.368 Omnibus: 10.497 Durbin-Watson: 2.166 Prob(Omnibus): 0.005 Jarque-Bera (JB): 18.606 Skew: 0.022 Prob(JB): 9.11e-05 Kurtosis: 3.960 Cond. No. 458. Giải thích 1 số thuật ngữ thống kê: R-squared: Độ giải thích biến phụ thuộc bởi các biến độc lâp R-squared = 0.565, 56.5% biến thiên của biến phụ thuộc(price) được giải thích bởi các biến độc lập, 43.5% còn lại nằm ngoài mô hình. Đa cộng tuyến: Xảy ra khi các biến độc lập có mối quan hệ tuyến tính cao (hệ số tương quan gần tới 1), biến không ảnh hưởng tới biến phụ thuộc. Có thể dùng P_value để tìm các biến bị đa cộng tuyến nếu P_value lớn hơn mức ý nghĩa thì biến đa cộng tuyến. Ở trường hợp của chúng ta biến bedrooms có P_value = 0.544, rất cao và có hiện tượng đa cộng tuyến Tự tương quan: Quan điểm thống kê cổ điển giả định rằng quan hệ tương quan giữa các thành viên của chuỗi của các quan sát được sắp xếp theo thời gian hoặc không gian trong ngữ cảnh hồi qui không tồn tại trong các nhiễu ui. Kiểm định Durbin-Watson = 2.166 cho kết quả nằm trong khoảng (1,3) nhiễu sẽ không có sự tự tương quan. Nghe có vẻ hơi rắc rối nhưng hãy quan sát đồ thị dưới đây để thấy rõ vấn đề: RMSE: Root Mean Square Error Là chênh lệch giữa giá trị thực và giá trị dự báo nhằm đánh giá chất lượng hay sự phù hợp của mô hình dự báo. rmse on test: 1.2007801545340961 rmse on train: 1.2521709451245022 Kỳ này chúng ta đã nói về cách xây dựng mô hình hồi quy tuyến tính để dự báo giá nhà. Tuy mô hình còn khá đơn giản và chúng ta cần phải tinh chỉnh rất nhiều thứ để có một mô hình tốt hơn, điều đó cần thời gian và sự nghiên cứu nghiêm túc. Theo mình data science là môn khoa học thực nghiệm, do vậy chúng ta phải kiên trì để giống như Edison - tìm ra chiếc bóng đèn của mình. Mời các bạn đón đọc kỳ sau, chúng ta sẽ nói về timeseries, một chủ đề khá thú vị đó !","categories":[],"tags":[{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"},{"name":"HCM-housing","slug":"HCM-housing","permalink":"https://quangminh49.github.io/tags/HCM-housing/"}]},{"title":"HCM housing chapter 0 - Intro","slug":"HCM housing chapter 0 intro ","date":"2020-03-09T17:00:00.000Z","updated":"2021-04-27T14:02:21.798Z","comments":true,"path":"2020/03/10/HCM housing chapter 0 intro /","link":"","permalink":"https://quangminh49.github.io/2020/03/10/HCM%20housing%20chapter%200%20intro%20/","excerpt":"Bất động sản là một lĩnh vực mình rất hứng thú, đặc biệt mấy năm gần đây tình hình bất động sản có sự tăng trưởng cực lớn, thị trường đã bùng nổ vào năm 2019, sang năm 2020 thì đã có sự chững lại cùng với đại dịch Covid 19. Thị trường biến động là thế nhưng chỉ ngồi nghe thì có lẽ chúng ta sẽ không bao giờ hiểu được chuyện gì đang diễn ra ngoài thị trường kia, bản thân tôi thì cũng không đủ tiền để trực tiếp tham gia thị trường vào lúc này, và sẽ thật đáng tiếc nếu bỏ qua gia đoạn đầy thách thức và cơ hội này. Nhưng vẫn còn hy vọng, data về các tin đăng rao bán bất động sản thì tràn ngập trên mạng, và may thay tôi cũng có chút kỹ năng để thu thập và xử lý chúng. Vì đó series HCM housing ra đời nhằm thỏa mãn niềm đam mê của bản thân và tìm ra các cơ hội đầu tư và cũng là một case study để thấy được data science có tính ứng dụng mạnh thế nào.","text":"Bất động sản là một lĩnh vực mình rất hứng thú, đặc biệt mấy năm gần đây tình hình bất động sản có sự tăng trưởng cực lớn, thị trường đã bùng nổ vào năm 2019, sang năm 2020 thì đã có sự chững lại cùng với đại dịch Covid 19. Thị trường biến động là thế nhưng chỉ ngồi nghe thì có lẽ chúng ta sẽ không bao giờ hiểu được chuyện gì đang diễn ra ngoài thị trường kia, bản thân tôi thì cũng không đủ tiền để trực tiếp tham gia thị trường vào lúc này, và sẽ thật đáng tiếc nếu bỏ qua gia đoạn đầy thách thức và cơ hội này. Nhưng vẫn còn hy vọng, data về các tin đăng rao bán bất động sản thì tràn ngập trên mạng, và may thay tôi cũng có chút kỹ năng để thu thập và xử lý chúng. Vì đó series HCM housing ra đời nhằm thỏa mãn niềm đam mê của bản thân và tìm ra các cơ hội đầu tư và cũng là một case study để thấy được data science có tính ứng dụng mạnh thế nào. Nguồn data trong toàn series mình crawl từ trang web https://batdongsan.com.vn/, code và data mình có để trên github của mình, nếu bạn nào hứng thú thì có thể clone về nghịch thử. Trong series này mình sẽ không nói đến việc crawl data mà chủ yếu đề cập đến xử lý và phân tích số liệu. Bước 1 : Clearn data 1 chút Đầu tiên bỏ cột không dùng tới lúc này đi cho gọn ( lúc này chưa dùng thôi nhé) Đổi tên cột nghe cho tây tây (và dễ gọi) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; font-size: 10px; &#125; .dataframe thead th &#123; text-align: right; &#125; 0 Loại tin rao Bán nhà riêng Địa chỉ Đường 13, Thủ Đức, Hồ Chí Minh Hướng nhà Đông-Bắc Số tầng 1(tầng) email lehoakhanhhoa87@gmail.com title Bán nhà đường 13, Linh Xuân, Nhà 1 trệt 1 lửng... Mã tin đăng 24908353 Loại hình tin đăng Tin thường Ngày đăng 2020-03-25 00:00:00 Ngày hết hạn 2020-01-04 00:00:00 long 106.756 lat 10.8533 district Thủ Đức street Linh Xuân price 2.75 tỷ area 81 m² uptime 25/03/2020 url /ban-nha-rieng-duong-13-phuong-linh-xuan/13-1-... Mặt tiền NaN Đường vào NaN Số phòng ngủ NaN Số toilet NaN Nội thất NaN Hướng ban công NaN Xem thông tin Non-Null và kiểu dữ liệu: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 14390 entries, 0 to 14389 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 floors 9656 non-null object 1 type 14390 non-null object 2 up_time 14390 non-null datetime64[ns] 3 end_time 14390 non-null datetime64[ns] 4 long 14388 non-null float64 5 lat 14388 non-null float64 6 district 14390 non-null object 7 street 14390 non-null object 8 price 14390 non-null object 9 area 14390 non-null object 10 facade 6603 non-null object 11 road_wide 7809 non-null object 12 bedrooms 8261 non-null object 13 toilets 6993 non-null float64 14 furniture 2593 non-null object dtypes: datetime64[ns](2), float64(3), object(10) memory usage: 1.6+ MB Đổi tên giá trị biến trong trường ‘type’ Tách giá price, area từ chuỗi: tỷ 13522 thuận 545 triệu 255 triệu/m² 68 Name: price_unit, dtype: int64 m² 14042 xác 348 Name: area_unit, dtype: int64 Chỉ lấy các record có ‘price_unit là’ ‘tỷ’ và ‘area_unit’ là ‘m²’ và chuyển dữ liệu ‘price’ và ‘area’ mới xử lý về kiểu float để tính toán: .dataframe tbody tr th:only-of-type { vertical-align: middle; font-size: 10px; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; long lat price area toilets count 13223.000000 13223.000000 13225.000000 13225.000000 6370.000000 mean 106.671245 10.811026 10.253711 88.636981 3.586185 std 0.059644 0.348298 177.415904 187.661826 2.892462 min 105.852447 10.387579 1.000000 8.000000 1.000000 25% 106.639569 10.768930 3.550000 48.000000 2.000000 50% 106.671258 10.797014 5.700000 64.000000 3.000000 75% 106.700338 10.832499 9.000000 90.000000 4.000000 max 106.954798 21.028949 19800.000000 12600.000000 70.000000 Tách giá trị số từ chuỗi các trường ‘floors’,’bedrooms’, ‘facade’, ‘road_wide’ .dataframe tbody tr th:only-of-type { vertical-align: middle; font-size: 10px; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; floors bedrooms facade road_wide 0 1(tầng) NaN NaN NaN 1 NaN NaN NaN NaN 2 2(tầng) 3(phòng) 4(m) 6,50(m) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 13225 entries, 0 to 14389 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 floors 8951 non-null object 1 bedrooms 7558 non-null object 2 facade 6118 non-null object 3 road_wide 7153 non-null object dtypes: object(4) memory usage: 516.6+ KB Nhận thấy giá trị số sẽ bao gồm các phần tử bắt đầu cho đến ký tự ‘(‘ trong chuỗi nên ta viết hàm xử lý như dưới: .dataframe tbody tr th:only-of-type { vertical-align: middle; font-size: 10px; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 0 Loại tin rao Bán nhà riêng Địa chỉ Đường 13, Thủ Đức, Hồ Chí Minh Hướng nhà Đông-Bắc Số tầng 1(tầng) email lehoakhanhhoa87@gmail.com title Bán nhà đường 13, Linh Xuân, Nhà 1 trệt 1 lửng... Mã tin đăng 24908353 Loại hình tin đăng Tin thường Ngày đăng 2020-03-25 00:00:00 Ngày hết hạn 2020-01-04 00:00:00 long 106.756 lat 10.8533 district Thủ Đức street Linh Xuân price 2.75 tỷ area 81 m² uptime 25/03/2020 url /ban-nha-rieng-duong-13-phuong-linh-xuan/13-1-... Mặt tiền NaN Đường vào NaN Số phòng ngủ NaN Số toilet NaN Nội thất NaN Hướng ban công NaN Bước 2 : Biểu diễn dữ liệu Dữ liệu khá đa dạng nhưng mình chỉ quan tâm tới các diện tích &lt; 200m2 và giá tiền &lt; 100 tỷ: Có 24 quận nên chia làm 4 dòng 6 cột vẽ cho đẹp Biểu đồ quan hệ giữa area và price theo từng quận : Các quận nội thành và phân bổ price: Tới đây thì dữ liệu cũng tàm tạm để xây dựng các mô hình dự đoán giá nhà rồi, và đó cũng sẽ là chủ để trong kỳ tới. Mời các bạn đón đọc !","categories":[],"tags":[{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"},{"name":"HCM-housing","slug":"HCM-housing","permalink":"https://quangminh49.github.io/tags/HCM-housing/"}]},{"title":"Credit Scorecard chapter 4 - Explain the Scorecard Model","slug":"Credit score chapter 4 ","date":"2020-03-03T17:00:00.000Z","updated":"2021-04-27T14:02:04.551Z","comments":true,"path":"2020/03/04/Credit score chapter 4 /","link":"","permalink":"https://quangminh49.github.io/2020/03/04/Credit%20score%20chapter%204%20/","excerpt":"Lê Ngọc Khả Nhi (ký danh) - một data scientist mà tôi ngưỡng mộ đã từng nói : “Một mô hình chính xác dĩ nhiên là tốt, nhưng chưa đủ. Độ chính xác của mô hình là tiêu chí được nhắm tới bởi hầu hết data scientist, tuy nhiên, tính chính xác không đồng nghĩa với tính hiệu quả. “ Thật vậy trong quá trình đưa mô hình Scorecard vào với vận hành tại thực tế công ty, tự bản thấy được còn có rất nhiều vấn đề cần được giải quyết !","text":"Lê Ngọc Khả Nhi (ký danh) - một data scientist mà tôi ngưỡng mộ đã từng nói : “Một mô hình chính xác dĩ nhiên là tốt, nhưng chưa đủ. Độ chính xác của mô hình là tiêu chí được nhắm tới bởi hầu hết data scientist, tuy nhiên, tính chính xác không đồng nghĩa với tính hiệu quả. “ Thật vậy trong quá trình đưa mô hình Scorecard vào với vận hành tại thực tế công ty, tự bản thấy được còn có rất nhiều vấn đề cần được giải quyết ! 1 Tại sao phải diễn giải mô hình Machine learning ?Cho đến thời điểm hiện tại, mô hình Machine learning đang được áp dụng để xây dựng Scorecard đã và đang cho những dự đoán rất chính xác về kết quả thu hồi nợ. Tuy nhiên khi đưa vào vận hành thì không ít người bắt đầu hoài nghi về kết quả dự đoán của mô hình, bao gồm những người vận hành có nhiều kinh nghiệm và một số chuyên viên phân tích. Họ bắt đầu đặt ra các câu hỏi hoài nghi : Tại sao một số trường hợp có thanh toán vào tháng trước nhưng vẫn được phân loại xấu (bin 8,9,10) ? Trường hợp khách hàng thanh toán rất đều tại sao lại xếp vào hợp đồng khó thu ? Tại sao một số trường hợp mọi yếu tố đều giống nhau nhưng chỉ vì có số POS thấp hơn mà hợp đồng bị phân loại kém hẳn ? Thời gian gần đây đã bắt đầu có những ý kiến hoài nghi, thậm chí cảnh báo về phong trào sử dụng Big data và Machine learning vào quá trình thu nợ. Một trong những luận điểm mà các bên phản đối đưa ra, đó là hầu hết những mô hình có nội dung không thể giải thích được. Khi họ không thể hiểu được cơ chế đằng sau kết quả, họ không thể tin cậy vào bản thân kết qủa đó, trong khi kết quả dự báo đôi khi ảnh hưởng rất nghiêm trọng đến kết quả thu nợ : thí dụ dự báo case xấu sẽ khiến nhân viên có tâm lý muốn bỏ ! Và cũng có vẻ bất công khi chúng ta đặt Target cao hơn cho 1 nhân viên nào đó vì một mô hình Machine learning đã dự báo rằng với danh mục của bạn thì phải đạt được con số như vậy. Nó cũng giống như một bệnh nhân đi khám và được bác sĩ giải thích như sau : “Bà có 99.5% nguy cơ đã mắc ung thư, vì một mô hình Neural network đã quyết định như vậy”. 2 Giải thích mô hình không đơn giản Trong Machine learning, luôn có một sự đánh đổi: “Tính tường minh của mô hình tỉ lệ nghịch với tính chính xác”, mô hình càng chính xác thì càng bí hiểm. Sự phức tạp đến từ bản chất của algorithm, số lượng input features hoặc cả 2. Thời đại Big Data và IOT đã cưỡng ép chúng ta phải lựa chọn những algorithm phức tạp nhất, vì những mô hình tuyến tính không có cách nào xử lý được lượng data lớn cỡ đó. Một số hình ảnh về sự phức tạp của các thuật toán Machine learning hiện đại: Cả 2 mô hình trên đều có cấu trúc vô cùng phức tạp nhằm mục đích bao quát dữ liệu theo nhiều chiều và rút trích được các tri thức trong đó. Trong sơ đồ trên, những mô hình chính xác nhất lại chính là những mô hình bí hiểm nhất, bao gồm: Random Forest, GBM, Deep neural network,… Chúng trở thành những hộp đen, hay mô hình “Bất khả Tri”. Thật không may, các mô hình chính để xây dụng cho Scorecard lại là Random Forest và neural network, việc giải thích chúng đã khó lại càng khó. Sơ đồ sau đây tóm tắt quy trình Suy diễn-Diễn dịch dựa vào mô hình: Để có thể thực hiện các ý tưởng kể trên chúng ta cần đặt ra các câu hỏi: Mô hình này chính xác đến đâu ? Có đáng tin cậy hay không ? Mô hình hoạt động như thế nào ? Nó đã học được gì từ dữ liệu ? Đây là câu hỏi rất quan trọng làm nền tảng cho việc diễn giải nội dung/cơ chế của mô hình. Giải đáp được câu hỏi này cho phép rút ra hàng loạt thông tin, suy diễn quan trọng, hữu ích bao gồm: Vai trò của mỗi biến ? Mối liên hệ bộ phận giữa mỗi biến và kết quả là gì ? Kết quả của việc giải thích cơ chế này còn có thể được sử dụng để quay ngược lại cải thiện mô hình bằng cách bỏ bớt những biến không quan trọng, tinh chỉnh tham số của algorithm, thêm dữ liệu mới. Giải thích được cơ chế của mô hình ở cấp độ cá thể: Mô hình hoạt động có chính xác không cho trường hợp này ? Tại sao kết quả lại như vậy ? Biến nào có vai trò/ ảnh hưởng quan trọng nhất ở cá thể này ? 3 Giải thích black-box model3.1 Local Interpretable Model-agnostic ExplanationsNhư đã nói ở trên các mô hình chúng ta đang sử dụng là Random Forest và Deep learning, do đó chúng ta không thể sử dụng các phương pháp tính điểm cho từng feature như Logistic hay Cây quyết định như Decision tree, … Phương pháp đầu tiên được áp dụng là LIME (Local Interpretable Model-agnostic Explanations), dựa trên giả định là bất kể mô hình phức tạp đến đâu, thì tại một miền cục bộ trong không gian dữ liệu, mô hình có thể được ước lượng xấp xỉ bằng quy luật tuyến tính. Đầu tiên, LIME sẽ lấy thông tin về đặc tính phân phối của feature dựa vào training dataset và nội dung trong model. Thông tin này được lưu trữ trong một object gọi là explainer. Việc diễn giải sẽ được áp dụng cho 1 trường hợp, cá thể mới. LIME sẽ mô phỏng một lượng lớn các trường hợp giả định (một đám mây nhiễu của features) nằm kề cận chung quanh trường hợp đang được xét, dựa vào quy luật phân phối của features mà nó đã ghi nhận từ trước. LIME áp dụng mô hình cho toàn bộ những điểm trong không gian nhiễu này, đồng thời tính khoảng cách giữa các điểm mô phỏng đến điểm trung tâm là trường hợp được xét. Khoảng cách này sẽ được chuyển thành thang điểm LIME chọn một số lượng M features tiêu biểu nhất cho phép mô tả tốt nhất khoảng cách nói trên. LIME dựng một mô hình rất đơn giản cho các điểm mô phỏng, sử dụng M features được chọn làm predictor, để giải nghĩa cho outcome của model. Mô hình này có dạng Tuyến tính, hoặc mô hình Decision tree. Tham số hồi quy cho mỗi Features được điều chỉnh bằng một trong số (Weight) tỉ lệ với khoảng cách sai biệt với giá trị feature có thực của cá thể. Việc diễn giải tính hợp lý của kết quả được thực hiện dựa vào Weight coefficient và danh sách M features được chọn. Nếu Weight coefficient &gt; 0, thì giá trị quan sát của feature đang ủng hộ cho kết quả tiên lượng (outcome) P, ngược lại, Weight Coefficient &lt;0 thì giá trị feature Mi chống lại kết quả P. Dưới đây là 1 ví dụ về Lime Cho B2: var value rule 0 DAY_FROM_LAST_PAID 4.0 DAY_FROM_LAST_PAID &lt;= 13.00 1 PAID_LAST_MONTH 1.0 PAID_LAST_MONTH &lt;= 1.00 2 LAST_RECEIPT_AMT 85410.0 LAST_RECEIPT_AMT &lt;= 1000000.00 3 PAID_COUNT_6M 1.0 PAID_COUNT_6M &lt;= 4.00 4 DPD 44.0 DPD &lt;= 44.00 5 DPD_1M 44.0 43.00 &lt; DPD_1M &lt;= 53.00 6 DPD_2M 14.0 DPD_2M &lt;= 17.00 7 POS_BOM 4222496.0 POS_BOM &lt;= 9471104.50 8 INSTALLMENT 704000.0 INSTALLMENT &lt;= 1403000.00 9 ACTIVE_NUM 2.0 1.00 &lt; ACTIVE_NUM &lt;= 2.00 10 DEFER_FLAG 0.0 DEFER_FLAG &lt;= 0.00 Trong 11 features có ảnh hưởng tới kết quả tiên lượng: 8 trong số đó phản đối cho kết quả “Paid” Các yếu tố ủng hộ: PAID_LAST_MONTH = 1 : có Paid vào tháng trước DPD = 44 &lt;= 44 INSTALLMENT = 703665 : INSTALLMENT vừa phải Nhìn vào các yếu tố phải đối thì có thể rút ra 1 số điều sau:Tuy có PAID_LAST_MONTH nhưng ngày thanh toán chỉ cách ngày 1 có 4 ngày, LAST_RECEIPT_AMT rất nhỏ chỉ có 85,410 =&gt; rất nghi ngờ đây là trường hợp đập tiền ! Mô hình đã tiên lượng trường hợp này vào bin 9, và đúng như dự báo, trường hợp này không có thanh toán trong tháng. 3.2 Descriptive Machine Learning EXplanationsDescriptive Machine Learning EXplanations là phương pháp được Przemyslaw Biecek công bố trên CRAN vào giữa tháng 6 năm 2018. Tác giả Biecek đã đi xa hơn bất cứ người nào khác trong việc diễn giải nội dung mô hình, với 3 ý tưởng độc đáo: Đưa ra một quy trình giải nghĩa phổ quát cho mọi mô hình, bất kể bản chất của algorithm và mục tiêu nghiên cứu (hồi quy/tiên lượng, phân loại hay suy diễn) Cho phép trình bày trực quan kết quả diễn giải của hàng loạt mô hình. Giải đáp hầu hết câu hỏi quan trọng để “hiểu” mô hình, bao gồm: Nội dung và cơ chế hoạt động : Tầm quan trọng của các biến, Quan hệ riêng phần của từng biến đối với kết quả (đặc biệt hữu ích cho bài toán hồi quy), độ chính xác của mô hình và diễn giải cho từng cá thể (theo phương pháp breakdown). Diễn giải cho từng cá thể:Phương pháp Descriptive Machine Learning EXplanations cho phép diễn giải mô hình cho mỗi cá thể bằng phương pháp phân rã mô hình theo kiểu breakdown. Mỗi feature trong mô hình được phân rã để thể hiện sự đóng góp của mình trong xác suất tiên lượng. Dưới đây là 1 ví dụ khác Cho B2: variable cumulative contribution sign position 0 intercept 0.272975 0.272975 1.0 20 1 DAY_FROM_LAST_PAID = 16.0 0.333200 0.060225 1.0 19 2 PAID_LAST_MONTH = 1.0 0.380553 0.047353 1.0 18 3 PAID_COUNT_6M = 6.0 0.384423 0.003870 1.0 17 4 MOB = 21.0 0.385874 0.001451 1.0 16 5 PRODUCT = PL 0.383132 -0.002742 -1.0 15 6 EFF_RATE = 46.99 0.380881 -0.002252 -1.0 14 7 ACTIVE_NUM = 1.0 0.381775 0.000895 1.0 13 8 PROVINCE = Tỉnh Vĩnh Long 0.383736 0.001960 1.0 12 9 PTP = 3.0 0.383736 0.000000 0.0 11 10 TENOR = 18.0 0.383736 0.000000 0.0 10 11 LAST_RECEIPT_AMT = 1699000.0 0.387844 0.004109 1.0 9 12 AGE = 38.0 0.383995 -0.003849 -1.0 8 13 DEFER_FLAG = 0.0 0.383900 -0.000095 -1.0 7 14 INSTALLMENT = 1687000.0 0.387731 0.003831 1.0 6 15 DPD_2M = 0.0 0.367192 -0.020538 -1.0 5 16 DPD = 53.0 0.352502 -0.014690 -1.0 4 17 DPD_3M = 0.0 0.320718 -0.031784 -1.0 3 18 DPD_1M = 23.0 0.272327 -0.048391 -1.0 2 19 POS_BOM = 576900.0 0.161821 -0.110506 -1.0 1 20 prediction 0.161821 0.161821 1.0 0 Lại một trường hợp có thanh toán trong tháng trước, và ngày thanh toán gần nhất là các đây 16 ngày. Có thể thấy khác hàng thanh toán đều với 6 lần thanh toán trong 6 tháng, DPD cách đây 2 và 3 tháng đều bằng 0. Tuy nhiên tháng trước và tháng này, DPD liên tục rớt xuống dù khách hàng có thanh toán. Dù thanh toán đều khách hàng vẫn chưa đóng đủ EMI nên hợp đồng vẫn trượt xuống trong khi POS còn rất thấp, có thể khách hàng chưa hiểu rõ hoặc không đồng tình với cách tính phí-lãi của công ty. Tình trạng trên đã kéo dài 2 tháng có thể tác động tiêu cực đến khách hàng nên trường hợp này được đưa vào trường hợp khó thu và được mô hình đưa ra xác suất thanh toán rất thấp chỉ 16.2 %. Tuy nhiên khi phân tích rõ vấn đề, các bộ phận, phòng ban liên quan hoàn toàn có thể đưa ra các hành động, giải pháp kịp thời và phù hợp để giải quyết các khúc mắc của khách hàng, và có thể nâng cao xác suất thu hơn. 4 Các trường hợp thực tế:B2: SCORE FIX_BIN PAID_LAST_MONTH DEFER_FLAG ddddd-ddddd 779 1 1 1 SCORE FIX_BIN PAID_LAST_MONTH DEFER_FLAG ccccccc-cccccccc 443 5 1 1 SCORE FIX_BIN PAID_LAST_MONTH DEFER_FLAG aaaaaa-aaaaaa 228 10 1 0 SCORE FIX_BIN PAID_LAST_MONTH DEFER_FLAG yyyyyyyy-yyyyyyyy 335 9 1 0 SCORE FIX_BIN PAID_LAST_MONTH DEFER_FLAG xxxxxxx-xxxxxxx 314 9 1 0 SCORE FIX_BIN PAID_LAST_MONTH DEFER_FLAG xxxxxxx-xxxxxxx 336 9 1 0","categories":[],"tags":[{"name":"Credit-Scorecard","slug":"Credit-Scorecard","permalink":"https://quangminh49.github.io/tags/Credit-Scorecard/"},{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"}]},{"title":"Credit Scorecard chapter 3 - Evaluation and Deployment","slug":"Credit score chapter 3 ","date":"2020-02-01T17:00:00.000Z","updated":"2021-04-27T14:01:45.392Z","comments":true,"path":"2020/02/02/Credit score chapter 3 /","link":"","permalink":"https://quangminh49.github.io/2020/02/02/Credit%20score%20chapter%203%20/","excerpt":"Ở bài trước chúng ta đã có thể xây dựng được mô hình scorecard rồi. Nhưng như vậy là chưa đủ, chúng ta cần trình bày, diễn giải nó một cách khoa học, và dễ hiểu để thuyết phục công ty đưa mô hình vào vận hành. Do vậy bài này mình sẽ nói về 1 số cách để diễn giải và trình mô hình cho sát với vận hành, với một số đầu mối trong bài các bạn hoàn toàn có thể tự build cho mình các report thật pro. Bài này kiến thức thì không có gì cao siêu nhưng hầu hết là do mình tự nghiền ngẫm và học hỏi. Do vậy rất mong nó có ích cho cách bạn dù ít hay nhiều.","text":"Ở bài trước chúng ta đã có thể xây dựng được mô hình scorecard rồi. Nhưng như vậy là chưa đủ, chúng ta cần trình bày, diễn giải nó một cách khoa học, và dễ hiểu để thuyết phục công ty đưa mô hình vào vận hành. Do vậy bài này mình sẽ nói về 1 số cách để diễn giải và trình mô hình cho sát với vận hành, với một số đầu mối trong bài các bạn hoàn toàn có thể tự build cho mình các report thật pro. Bài này kiến thức thì không có gì cao siêu nhưng hầu hết là do mình tự nghiền ngẫm và học hỏi. Do vậy rất mong nó có ích cho cách bạn dù ít hay nhiều. Mục tiêu gồm : Giới thiệu các tiêu chuẩn đánh giá mô hình Các cách triển khai mô hình Đầu tiên build lại mô hình giống kỳ trướcTrong bài này sẽ đề cập đến cách diễn giải và trình bày là chính, do vậy ta sẽ dùng mô hình LogisticRegression để làm ví dụ cho đơn giản Dự báo với mô hìnhDự đoán với phương thức predict(): phương thức predict cho ta kết quả đầu ra với nhãn cụ thể : array([0., 0., 0., ..., 0., 0., 0.]) Một cách dễ hiểu nếu kết quả là : 0 tương ứng với khách hàng bình thường 1 tương ứng với khách hàng mục tiêu Phân bổ của các lớp thực tế: Phân bổ của các lớp theo dự báo: Nhìn vào pie chart ta thấy được tỷ lệ giữa 2 class 0, 1 trong kết quả dự báo của tệp test có sự khác biệt so với thực tế. Có vẻ như mô hình dự báo chưa chính xác. Nhưng làm sao để biết được mô hinh chúng ta có hiệu quả và hiệu quả đến bao nhiêu. Chúng ta có thể đánh giá mô hình theo một số phương pháp dưới đây Các phương pháp đánh giá mô hình:Trong các bài toán phân lớp nhị phân (chỉ có 2 lớp) có một lớp quan trọng hơn và cần dự báo chính xác thì lớp quan trọng hơn sẽ được gán nhãn là 1 Positive, biến còn lại là 0: Negative Chúng ta cần làm quen với các khái niệm : TP - True Positive: Số lượng dự đoán là 1 và thực tế là 1 FP - False Positive: Số lượng dự đoán là 1 và thực tế là 1 TN - True Negative: Số lượng dự đoán là 0 và thực tế là 0 FN - False Negative: Số lượng dự đoán là 0 và thực tế là 1 FPR - False Positive Rate: Tỷ lệ dự báo sai biến Positive FNR - False Negative Rate: Tỷ lệ bỏ sót biến Positive -confusion_matrix: thể hiện có bao nhiêu dữ liệu được dự đoán vào class nào và class thực sự của chúng. Accuracy: = TP+TN / TP+TN+FP+FN (Tỷ lệ dự đoán đúng ) Recall: = TP / TP+FN (Tỉ lệ Dự đoán true positive trong số những điểm thực sự là positive (TP + FN).) Precision: = TP / TP+FP (Tỷ lệ số điểm true positive trong số những điểm được phân loại là positive (TP + FP)) confusion_matrix: Các chỉ số: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; metrix accuracy recall precision score 0.812 0.265501 0.620818 Quay lại một chút về vấn đề dự đoán với phương thức predict() để hiểu hơn tại sao mô hình lại có thể dự báo nhãn cho tệp test. Giá trị đầu ra của hàm sigmoid luôn nằm trong khoẳng từ 0 đến 1 nếu ta lấy ngưỡng(threshold) là 0.5 và chia dữ liệu làm 2 phầnphần &lt; 0.5 sẽ được gán nhãn là 0 và ngược lại thì ta sẽ được kết quả như phương thức predict() đã làm. Phương thức predict_proba() trả về cho ta giá trị xác xuất, phương thức predict() thì lấy ngưỡng mặc định là 0.5 và đưa ra kết quả phân lớp. Do vậy nếu ta sử dụng phương thức predict_proba() và thay đổi ngưỡng phân lớp thì kết quả phân lớp sẽ khác biệt rất nhiều, confusion_matrix, accuracy, recall, precision cũng sẽ thay đổi theo. Vì mục việc tìm ra các khách hàng không thanh toán - label 1 là quan trọng hơn nên chung ta sẽ cố gắng tăng FPR để giảm FNR, với tôn chỉ là thà giết lầm còn hơn bỏ sót. Bằng cách giảm ngưỡng(threshold) xuống dưới 0.5(vd: 0.2) chúng ta sẽ làm tăng lượng dữ liệu được gán nhãn Positive lên và TP, FP sẽ tăng , TN, FN sẽ giảm theo - đồng nghĩa với giá trị FPR sẽ tăng Receiver operating characteristic - ROC curve sẽ thể hiện được FPR và TPR thay đổi thế nào qua các threshold thư viện sklearnhỗ trợ chúng ta khảo sát và vẽ ROC curve rất tốt. Vẽ ROC curve và thể hiện 3 ví dụ trên biểu đồ: threshold:0.45, FPR:0.06, TPR:0.31 threshold:0.13, FPR:0.45, TPR:0.75 threshold:0.07, FPR:0.85, TPR:0.97 AUC - Area Under the Curve là diện tích nằm dưới ROC curve cũng là một tiêu chuẩn đánh giá mô hình, giá trị này càng cao thì mô hình càng tốt. Nếu thuộc lĩnh vực tài chính thì các bạn sẽ không lạ gì với chỉ số GINI - tiêu chuẩn để đo độ chính xác của mô hình credit scorecard. Và chúng ta có thể tính GINI theo công thức đơn giản dưới đây: GINI = AUC * 2 - 1 Các phương pháp triển khai mô hình:Sau khi chúng ta xây dựng được mô hình thì vấn đề triển khai thế nào cho dễ dàng hỗ trợ cho vận hành tốt nhất cũng là vấn đề quan trọng Bước 1: Chuyển dải xác xuất về score:Đầu tiên khi Chúng ta dùng phương thức predict_proba() để chấm điểm thì kết quả trả về sẽ là một dải xác xuất, ta hoàn toàn có thể dùng dải xác xuất này là điểm số để đánh giá chất lượng của từng trường hợp. Tuy nhiên con số xác xuất thì sẽ không được mỹ quan và thân thiện với các bộ phận vận hành vì vậy chúng ta có thể chuển dải xác xuất về dải điểm từ 0 đến 1000 Dữ liệu thường có phân phối lệch trái do số giá trị có class 0 chiếm áp đảo, cho nên điều quan trọng hơn hết là khi ta chuyển dải xác xuất sang dải score ta có thể tổ chức lại phân phối của dải điểm để gần với phân phối chuẩn. Phân phối của dải score cân xứng hơn hẳn, sẽ phù hợp cho ứng dụng, hỗ trợ vận hành hơn dải xác xuất. Bước 2: Chuyển dải score về các bin:Tuy dải score đã đẹp rồi, nhưng ta vẫn cần khái quát chúng hơn nữa để có thể dễ dàng theo dõi performance dễ dàng hơn. Trường hợp dưới đây mình gon dải score vào 5 bin sao cho số lượng phần tử mỗi bin xấp xỉ nhau, từ đó ta có thể đễ dàng phân tích theo nhiều chiều hơn. Với cách thức trên thì việc đưa mô hình vào vận hành là rất thuận tiện rồi. Bài kỳ này xin kết thúc tại đây. Kỳ sau mình sẽ nói về cách đánh giá lại hiệu quả ứng dụng mô hình và tối ưu vận hành qua mô hình nhân quả.","categories":[],"tags":[{"name":"Credit-Scorecard","slug":"Credit-Scorecard","permalink":"https://quangminh49.github.io/tags/Credit-Scorecard/"},{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"}]},{"title":"Credit Scorecard chapter 2 - Machine learning","slug":"Credit score chapter 2 ","date":"2020-01-19T17:00:00.000Z","updated":"2021-04-27T14:01:55.278Z","comments":true,"path":"2020/01/20/Credit score chapter 2 /","link":"","permalink":"https://quangminh49.github.io/2020/01/20/Credit%20score%20chapter%202%20/","excerpt":"Bài viết thứ 2 nằm trong series về Credit Scorecard trong các tổ chức tín dụng. Trong bài này chúng ta cùng tìm hiểu cách ứng dụng các mô hình Machine trong chấm điểm tín dụng cũng như tìm ra mô hình tối ưu nhất.","text":"Bài viết thứ 2 nằm trong series về Credit Scorecard trong các tổ chức tín dụng. Trong bài này chúng ta cùng tìm hiểu cách ứng dụng các mô hình Machine trong chấm điểm tín dụng cũng như tìm ra mô hình tối ưu nhất. Credit Scorecard với mục tiêu là chấm điểm và phân loại khách hàng, cho phép đưa ra quyết định về chính sách can thiệp nhằm tối ưu hóa hiệu quả kinh doanh. Mục tiêu gồm : Các phương pháp chuyển dạng, chuẩn hóa dữ liệu Giới thiệu các thuật toán machine learning cơ bản Turning model sử dụng GridSearchCV và tìm ra mô hình tối ưu Bước 1: Data preprocessingDo dữ liệu đã được clean nên chúng ta đến thẳng bước tranform data để chuẩn bị training model machine learning OneHotEncoderKỹ thuật onehot (hay dummy) là kỹ thuật chuyển dạng dữ liệu từ dạng chuỗi sang dạng bool mà vẫn giữ nguyên được giá trị thông tin của dữ liệu. Với phương thức là với mỗi giá trị trong trường dữ liệu cũ ta tạo một trường dữ liệu mới thể hiện rằng dữ liệu tại đó có bằng giá trị đó không. Chúng ta sẽ thấy rõ nó hoạt động thế nào sau bước dưới đây: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; field2 field3 field9 field10 field13 field35 field38 0 field2_v0 field3_v0 field9_v0 field10_v0 field13_v0 field35_v0 field38_v0 1 field2_v1 field3_v0 field9_v0 field10_v1 field13_v1 field35_v0 field38_v1 2 field2_v2 field3_v1 field9_v0 field10_v2 field13_v2 field35_v0 field38_v2 data sau khi OneHotEncoder: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; field2_field2_v0 field2_field2_v1 field2_field2_v2 0 1.0 0.0 0.0 1 0.0 1.0 0.0 2 0.0 0.0 1.0 MinMaxScalerLà kỹ thuật chuẩn hóa dữ liệu phổ biến cho các dữ liệu Numeric. Với cơ chế là đưa dữ liệu về 1 thang đo mớivd: MinMaxScaler(feature_range=(-1,1)) -1 : giá trị nhỏ nhất 1 : giá trị lớn nhất Tại sao chuẩn hóa dữ liệu lại cẩn thiết ? Vì khi tính toán với nhiều trường dữ liệu khác nhau thì sẽ có các phép toán kiểu như : age + loanGiả sử khách hàng 45 tuổi và vay 50 triệu thì việc lấy số 45 tuổi + 50 triệu có sự bất đồng lớn về thang đo tính toán Do vậy ta cần đưa age và loan về cùng thang đo (-1,1) để đạt được hiệu quả cao khi traing model Bước 2 : MODELSau khi dữ liệu đã sạch sẽ thơm tho thì chún ta có thể đem đi xào nấu thỏa thích với đủ thể loại thuật toán machine learning Việc đầu tiên chúng ta cần làm là chia tệp dữ liệu thành các bộ traing_set và test_set: Define models and gridsearchsChỉ có 1 cách để biết được mô hình nào là tốt nhất là thử nghiệm thật nhiều loại mô hình. Nhưng lại có một vấn để như sau: Giả sử ta muốn tìm mô hình tốt hơn giữ mô hình Logistic và mô hình KNN, vậy đầu tiên ta phải tìm được mô hình Logistic tốt nhất và mô hình KNN tốt nhất, sau đó mới so sánh chúng với nhau để tìm ra cái tốt hơn trong những cái tốt. Nghe hơi rối não nhưng ở đây chúng ta sẽ đề cạp đến 2 khái niệm để giải thích vấn đề này: Parameter vs Hyper Parameter: Parameter : Tham số trong các phương trình tối ưu của thuật toán ,vd: khi chúng ta xét hàm tuyến tính: f(x) = a*x + b thì a và b chính là các Parameter =&gt; chúng ta cần tìm các parameter để mô hình fit với tập dữ liệu Hyper Parameter: Siêu tham số trong các mô hình machine learningvd: khi chúng ta định nghĩa mô hình RandomForestClassifier(max_features=’sqrt’, criterion=’gini’)Thì max_features và criterion là các hyper parameter của mô hình là các rule, thuật toán cụ thể mô hình sẽ sử dụng =&gt; chúng ta cần tìm các hyper parameter để tìm ra bộ setup tốt nhất cho mô hình machine learningGridSearchCV tìm kiếm hyper parameter tối ưu qua bộ param_grid truyền vào bằng cách chạy từng trường hợp của bộparam và lựa chọn params cho score tốt nhất Thực hiện GridSearchCV cho từng thuật toán và so sánh kết quả theo AUC .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; classifier auc best_params 0 KNN 0.697754 {'n_neighbors': 20} 1 SVM 0.744170 {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'} 2 RF 0.773504 {'criterion': 'entropy', 'max_depth': 12, 'max... 3 MLP 0.653790 {'alpha': 0.01, 'hidden_layer_sizes': 17, 'max... 4 Ada 0.724699 {'algorithm': 'SAMME', 'base_estimator__max_de... Model RF: RandomForestClassifier() cho kết quả AUC cao nhất : 0.77 do vậy ta dùng bộ best params của mô hình của RF để train mô hình dự báo chính của chúng ta. RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;entropy&#39;, max_depth=12, max_features=&#39;log2&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) Thêm các metrix để đánh giá mô hình: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; metrix accuracy f1 recall precision auc gini ks score 0.821667 0.387171 0.26868 0.692623 0.769251 0.538503 0.417165 Do chúng ta đang cố gắp phân loại khách hàng vào 2 nhóm, do vậy chỉ số AUC và KS sẽ được ưu tiên dùng trong việc đánh giá mô hình Với mô hình RandomForestClassifier, chúng ta có thể lấy được độ quan trọng của từng biến tới mô hình: Vậy là tới đây chúng ta đã biết cách để tìm ra mô hình machine learning tối ưu để giải quyết vấn đề của mình, bài cũng khá dài nên chapter này sẽ tạm dừng tại đây. Kỳ tới chúng ta sẽ đi sâu hơn vào các ứng dụng của machine learning cũng như cách khai thác chúng !","categories":[],"tags":[{"name":"Credit-Scorecard","slug":"Credit-Scorecard","permalink":"https://quangminh49.github.io/tags/Credit-Scorecard/"},{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"}]},{"title":"Credit Scorecard chapter 1 - Logistic Regression and WOE-IV","slug":"Credit score chapter 1 ","date":"2020-01-03T17:00:00.000Z","updated":"2021-04-27T14:01:58.982Z","comments":true,"path":"2020/01/04/Credit score chapter 1 /","link":"","permalink":"https://quangminh49.github.io/2020/01/04/Credit%20score%20chapter%201%20/","excerpt":"Credit Scorecard với mục tiêu là chấm điểm và phân loại khách hàng, cho phép đưa ra quyết định về chính sách can thiệp nhằm tối ưu hóa hiệu quả kinh doanh. Thí dụ minh họa là một dữ liệu về thông tin cơ bản cũng như lịch sử hành vi của khách hàng thuộc nhóm trễ hạn nợ 2 tháng (B2_BOM). Bài toán được giải quyết dựa trên dữ liệu của 10000 khách hàng, bao gồm nhiều thông tin, các thông tin đã được biến đổi cũng như thay đổi tên trường để đảm bảo lý do bảo mật.","text":"Credit Scorecard với mục tiêu là chấm điểm và phân loại khách hàng, cho phép đưa ra quyết định về chính sách can thiệp nhằm tối ưu hóa hiệu quả kinh doanh. Thí dụ minh họa là một dữ liệu về thông tin cơ bản cũng như lịch sử hành vi của khách hàng thuộc nhóm trễ hạn nợ 2 tháng (B2_BOM). Bài toán được giải quyết dựa trên dữ liệu của 10000 khách hàng, bao gồm nhiều thông tin, các thông tin đã được biến đổi cũng như thay đổi tên trường để đảm bảo lý do bảo mật. Biến output : label label = 1: khách hàng mục tiêu label = 0: khách hàng bình thường Mục tiêu bài viết : Trực quan hóa dữ liệu với seaborn Giới thiệu thuật toán Logistic regression với WOE Biểu đồ trực quan hóa dữ liệuCách để chúng ta hiểu dữ liệu nhanh nhất đó chính là qua hình ảnh Dưới đây mình chủ yếu sử dụng thư viện seaborn vì nó khá thuận tiện mà lại rất đẹp nữa Biểu đồ histogram để thấy được tần suất của dữ liệu: Biến phân loại: Biểu đồ pairplot để thấy được mối quan hệ của dữ liệu: Quan sát thêm sự mối quan hệ giữa các biến với sns.jointplot: factor “field35” theo “field14” và “label”: Giới thiệu :Sau khi nhìn qua dữ liệu chắc chúng ta ai cũng muốn hiểu sâu hơn về dữ liệu cũng như khai thác tri thức từ chúng. Một phương pháp thống kê phổ biến dễ hiểu và cũng rất hiệu quả cho các bài toán phân lớp nhị phân thường được nhắc tới đó chính là Logistic regression. Mô hình hồi quy logistic là một trong những kỹ thuật thống kê được sử dụng phổ biến nhất để giải quyết vấn đề phân loại nhị phân. Nó được chấp nhận trong hầu hết các lĩnh vực. Hai khái niệm này - Weight of Evidence (WOE) và Information Value (IV) phát triển từ cùng một kỹ thuật hồi quy logistic. Hai thuật ngữ này đã tồn tại trong thế giới chấm điểm tín dụng trong hơn 4-5 thập kỷ. Chúng đã được sử dụng làm chuẩn để sàng lọc các biến trong các dự án mô hình rủi ro tín dụng như xác suất vỡ nợ. Chúng giúp khám phá dữ liệu và các biến. Nó cũng được sử dụng trong dự án phân tích tiếp thị như mô hình phân khúc khách hàng, v.v. mô hình Logistic regression :Với cơ chế phân lớp đễ hiểu và các hàm kích hoạt cơ bản (activation) như sigmoid thì chúng ta có thể khai thác được rất nhiều từ dữ liệu Về chi tiết cụ thể về cơ chế hoạt động của Logistic regression và hàm activation sigmoid thì mời các bạn các bạn đón đọc trang web rất uy tín dưới đây machinelearningcoban.com Weight of Evidence(WOE)WOE = In(% of non-events / % of events) WOE : Trọng số cho biết sức mạnh dự đoán của một biến độc lập liên quan đến biến phụ thuộc. Giúp chuyển đổi một biến độc lập liên tục thành một tập hợp các nhóm hoặc bin dựa trên sự giống nhau của phân phối biến phụ thuộc. Information Value(IV):IV = ∑ (% of non-events - % of events) * WOE IV là một trong những kỹ thuật hữu ích nhất để chọn các biến quan trọng trong mô hình dự đoán. Nó giúp xếp hạng các biến trên cơ sở tầm quan trọng của chúng. Các bạn có thể tham khảo thêm về WOE và IV qua các bài viết dưới đây medium.comkaggle.com Thực hành nào :Lý thuyết vậy là nhiều rồi, chắc đọc cũng chán ngấy cả người. Vậy chúng ta cùng thực hành chút cho nóng người nào. Về phương pháp biến đổi WOE thì có rất nhiều, mình cũng tự custom module để tính woe để theo ý mình nhưng cũng cần kết hợp với các thư viện có sẵn để đạt kết quả tốt nhât: Dưới đây là kết quả chúng chạy mô hình credit scorecard Vậy là chúng ta đã đi lướt qua khá nhiều kiến thức. Ở kỳ sau chúng ta sẽ áp dụng nhiều phương pháp và mô hình machine learning hiện đại vào trong bài toán credit scorecard.","categories":[],"tags":[{"name":"Credit-Scorecard","slug":"Credit-Scorecard","permalink":"https://quangminh49.github.io/tags/Credit-Scorecard/"},{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"}]},{"title":"Credit Scorecard chapter 0 - Intro","slug":"Credit score chapter 0 ","date":"2020-01-02T17:00:00.000Z","updated":"2021-04-27T14:02:02.363Z","comments":true,"path":"2020/01/03/Credit score chapter 0 /","link":"","permalink":"https://quangminh49.github.io/2020/01/03/Credit%20score%20chapter%200%20/","excerpt":"Credit Scorecard hay Credit Scoring Model là một trong số những mô hình lượnghóa rủi ro tín dụng cơ bản thường được sử dụng nhất. Với mục đính đánh giá các khả năng của người vay như vỡ nợ hay không vỡ nợ, vi phạm luật hay không, …Credit Scorecard đưa ra một dải điểm để đáng giá tệp khách hàng của tổ chức tín dụngSố điểm càng cao, thì người vay có xác suất vỡ nợ càng thấp và ngược lại.","text":"Credit Scorecard hay Credit Scoring Model là một trong số những mô hình lượnghóa rủi ro tín dụng cơ bản thường được sử dụng nhất. Với mục đính đánh giá các khả năng của người vay như vỡ nợ hay không vỡ nợ, vi phạm luật hay không, …Credit Scorecard đưa ra một dải điểm để đáng giá tệp khách hàng của tổ chức tín dụngSố điểm càng cao, thì người vay có xác suất vỡ nợ càng thấp và ngược lại. Ứng dụng :Xác định rủi ro tín dụng để đưa ra các phương án cụ thể: Có phê duyệt khoản vay hay không. Được giải ngân bao nhiêu. Vay với lãi suất bao nhiêu. Cần những tài sản đảm bảo gì. Cách thức chăm sóc ra sao . Ngoài ra còn vô vàn ứng dụng khác mà đối với từng doanh nghiệp và trường hợp cụ thể. Bản thân mình là người làm trong lĩnh vực Credit Scorecard tuy rằng chưa lâu nhưngcũng có một số dự án đã và đang được ứng dụng. Đó cũng là một may mắn lớn của mình. Một số dự án của mình:Scorecard cho nhóm nợ B3 Loan: Scorecard cho sản phẩm Card: Scorecard cho nhóm nợ B1 và B2 Loan: Một số report mình build để theo dõi performance theo mô hình: Rất mong các bạn theo dõc các kỳ sau để hiểu hơn về cách xây dựng một mô hình Credit scorecard. Xin cám ơn !","categories":[],"tags":[{"name":"Credit-Scorecard","slug":"Credit-Scorecard","permalink":"https://quangminh49.github.io/tags/Credit-Scorecard/"},{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"}]},{"title":"Webapp with Google Ecosystem","slug":"google_app","date":"2019-12-27T17:00:00.000Z","updated":"2021-04-27T14:02:12.615Z","comments":true,"path":"2019/12/28/google_app/","link":"","permalink":"https://quangminh49.github.io/2019/12/28/google_app/","excerpt":"“Không biết thì hỏi Google” bao lâu nay Google đã đi vào đời sông của chúng ta như một phần không thể thiếu như vậy.Chúng ta vẫn ngày ngày sử dụng các tiện ích của anh Google và bản thân tôi luôn cảm thấy biết ơn vì điều đó, Gmail,Drive, Youtube, Google Spreadsheet…","text":"“Không biết thì hỏi Google” bao lâu nay Google đã đi vào đời sông của chúng ta như một phần không thể thiếu như vậy.Chúng ta vẫn ngày ngày sử dụng các tiện ích của anh Google và bản thân tôi luôn cảm thấy biết ơn vì điều đó, Gmail,Drive, Youtube, Google Spreadsheet… Nhưng hệ sinh thái Google cho chúng ta nhiều hơn thế Chúng ta có thể tự build cho mình một Webapp trên nền tảng Google Ecosystem. Các bạn không nghe lầm đâu, chính là 1 webapp như quản lý phòng khách sạn, quản lý quán cafe hoàn toàn miễn phí và rất đơn giản. Chuyện là trước đây vài năm mình có tìm kiếm một công cụ nhanh gọn, miễn phí để xây dựng 1 ứng dụng quả lý đơn giản cho mình như quản lý chi tiêu, quản lý cửa hàng nhỏ, điều đầu tiên mình nghĩ tới là Excel VBA. Với Excel thì quá đơn giản để thực hiện những ý tưởng của mình cộng với VBA thì như hổ thêm cánh, thế là ứng dụng cơ bản ra đời. Nhưng ngay sau đó mình nhận ra một điều vô cùng bất tiện là khi mình không mang theo laptop thì không thể sử dụng ứng dụng, đó là lúc mình nghĩ mình cần 1 Webapp. Sau khi tìm kiếm trên mạng 1 hồi thì cuối cùng giải pháp từ chính cái anh mà mình đang dùng để tìm kiếm - Google. Với công thức gồm có Google Spreadsheet + Google Apps Script + Google Data studio mình đã build Webapp vượt qua mong đợi của mình về cả tính năng và giao diện. Google Spreadsheet thì chắc các bạn ai cũng biết rồi mình sẽ không giới thiệu nữa. Google Apps Script là công cụ viết mã kịch bản của Google sử dụng ngôn ngữ Javascript nó giống như VBA của excel vậy. Google Data studio là công cụ BI google, tuy rằng không lâu đời như các công cụ BI khác nhưng nó có tính tương thích cao vì cùng một nhà với các công cụ trên. Do vậy build dashboard, report bằng google data studio phải nói là tuyệt cú mèo Ý tưởng Một web app đơn giảnGoogle Apps Script cho phép chúng ta dễ dàng xây dựng front-end cho web bằng định dạng html, css và javascripts, phần front-end như vật đã rất đảm bảo. Nhưng vì mục tiêu của chúng ta là xâp dựng 1 web app nên phần back-end và cơ sở dữ liệu là không thể thiếu.Do quy mô và chức năng của web app rất nhỏ nên google sheets hoàn toàn có thể đáp ứng là cơ sở dữ liệu. Ngoài ra Google Spreadsheet sẽ giúp ta tính toán các chức năng và nhiệm vụ cơ bản, kết hợp với javascript thì coi như nhiệm vụ back-end có thể hoàn thànhVì cơ sở dữ liệu xây dựng trên Google Spreadsheet nên việc xây dựng Dashboard, report trên Google Data studio là rất dễ dàng. Ta hoàn toàn có thể nhúng các kết quả từ Google Data studio vào web app của mình để làm phong phú thêm chức năng.Với ý tưởng đơn giản như vậy mình đã cho ra đời rất nhiều ứng dụng web app, có đủ chức năng thêm, xóa, sửa, reports phục vụ cho bản thân và những người bạn của mình. Các ứng dụng:1. Ứng dụng chi tiêu cá nhân :Thực hiên việc gi chép các phát sinh chi tiêu , thống kê các chi tiêu và đặt ngân sách chi tiêu 2. Ứng dụng chi tiêu chungMình từng ở chung cùng 3 bạn nữa hồi đại học nên có rất nhiều khoản chi tiêu chung, do vậy mình làm ứng dụng để thực hiện các chức năng phân bổ công nợ cho mỗi người và gửi mail tự động nhắc nợ mỗi cuối tuần.","categories":[],"tags":[{"name":"Google-Ecosystem","slug":"Google-Ecosystem","permalink":"https://quangminh49.github.io/tags/Google-Ecosystem/"}]},{"title":"Estimete ACR","slug":"statistic_0","date":"2019-10-02T17:00:00.000Z","updated":"2021-04-27T14:02:44.728Z","comments":true,"path":"2019/10/03/statistic_0/","link":"","permalink":"https://quangminh49.github.io/2019/10/03/statistic_0/","excerpt":"Trong quá trình vận hành thu nợ số lượng hợp đồng mỗi nhân viên phụ trách được gọi là ACR. Số ACR có ảnh hưởng rất lớn đến lợi ích cho công ty, cụ thể là nếu ACR quá lớn thì hiệu quả thu nợ không cao, nếu ACR quá nhỏ thì sẽ cần lượng nhân sự lớn hơn nên chi phí cho nhân sự sẽ cao hơn. Quan trọng nhất là ở các nhóm nợ cao, công ty sẽ phải trích lập dự phòng theo tỷ lệ nhất định, do vậy việc thu nợ tốt sẽ giảm thiểu được rất nhiều chi phí, tăng lợi nhuận cho công ty.","text":"Trong quá trình vận hành thu nợ số lượng hợp đồng mỗi nhân viên phụ trách được gọi là ACR. Số ACR có ảnh hưởng rất lớn đến lợi ích cho công ty, cụ thể là nếu ACR quá lớn thì hiệu quả thu nợ không cao, nếu ACR quá nhỏ thì sẽ cần lượng nhân sự lớn hơn nên chi phí cho nhân sự sẽ cao hơn. Quan trọng nhất là ở các nhóm nợ cao, công ty sẽ phải trích lập dự phòng theo tỷ lệ nhất định, do vậy việc thu nợ tốt sẽ giảm thiểu được rất nhiều chi phí, tăng lợi nhuận cho công ty.","categories":[],"tags":[{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"},{"name":"Statistic","slug":"Statistic","permalink":"https://quangminh49.github.io/tags/Statistic/"}]},{"title":"Object Detection Model - MASK RCNN","slug":"mask-rcnn","date":"2019-10-02T17:00:00.000Z","updated":"2021-04-27T14:02:41.175Z","comments":true,"path":"2019/10/03/mask-rcnn/","link":"","permalink":"https://quangminh49.github.io/2019/10/03/mask-rcnn/","excerpt":"Đến với computer vision, chúng ta có rất nhiều ứng dụng trong thực tế, rất nhiều trang web, blog đã trình bày cách thực hiện các hướng dẫn với các pre-trained model. Tuy nhiên vì do các model không phải do chính chúng ta build nên chúng ta phải tuân theo các cấu trúc, định dạng của tác giả, và thường thì khi chúng ta xây dựng một dự án mà muốn Object Detection là một phần trong đó thì đây chính phần khá khoai. Do vậy hôm nay mình viết về việc custom lại một pre-trained model trên Keras và việc sử dụng lại nó là vô cùng dễ dàng, rất thích hợp để trở thành một module trong các project của mình.","text":"Đến với computer vision, chúng ta có rất nhiều ứng dụng trong thực tế, rất nhiều trang web, blog đã trình bày cách thực hiện các hướng dẫn với các pre-trained model. Tuy nhiên vì do các model không phải do chính chúng ta build nên chúng ta phải tuân theo các cấu trúc, định dạng của tác giả, và thường thì khi chúng ta xây dựng một dự án mà muốn Object Detection là một phần trong đó thì đây chính phần khá khoai. Do vậy hôm nay mình viết về việc custom lại một pre-trained model trên Keras và việc sử dụng lại nó là vô cùng dễ dàng, rất thích hợp để trở thành một module trong các project của mình. Dể minh họa, mình sẽ sử dụng pre-train model Mask-RCNN để tìm vợt tenis trong hình. Các bước thực hiện Step 1 : Clone Mask RCNN trên GitHub Repository :1git clone https://github.com/matterport/Mask_RCNN.git Step 2 : Cài đặt thư viện Mask RCNN:12cd Mask_RCNNpython setup.py install Step 3 : chuẩn bị dữ liệu: chúng ta cần tải 1 số hình ảnh có vợt tenis về và gán nhãn cho chúng bằng labelimg, càng lấy nhiều hình thì kết quả càng chính xác, và tất nhiên thời gian train model sẽ càng nhiều hơn Step 4 : Chaỵ code và tận hưởng kết quả:code mình để dưới này mọi người có thể copy về chạy thử link colab Và đây là kết quả. Cũng khá khả quan phải không ạ!","categories":[],"tags":[{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"},{"name":"Computer-Vision","slug":"Computer-Vision","permalink":"https://quangminh49.github.io/tags/Computer-Vision/"}]},{"title":"Breaking captcha CIC","slug":"breaking captcha","date":"2019-08-25T03:17:55.000Z","updated":"2021-04-27T14:01:10.229Z","comments":true,"path":"2019/08/25/breaking captcha/","link":"","permalink":"https://quangminh49.github.io/2019/08/25/breaking%20captcha/","excerpt":"Thời buổi bây giờ các thuật toán deep learning đã rất phát triển, các pre-trained model thì vô số, việc tiếp cận cũng rất dễ dàng. Tự thấy mình cũng chẳng là gì trong thế giới trí tuệ đó, nhưng mình lại rất thích đem các phát kiến machine learning đó vào ứng dụng trong thực tế. Cũng tự nhủ với mình rằng mình không phát minh ra các thuật toán đó nhưng mình sẽ ứng dụng chúng thật tốt để tạo ra thêm nhiều giá trị cho xã hội. Bài này mình sẽ giới thiệu một ứng dụng nho nhỏ của mình về Computer Vision trong việc tự động các công việc nhàm chán tại công ty.","text":"Thời buổi bây giờ các thuật toán deep learning đã rất phát triển, các pre-trained model thì vô số, việc tiếp cận cũng rất dễ dàng. Tự thấy mình cũng chẳng là gì trong thế giới trí tuệ đó, nhưng mình lại rất thích đem các phát kiến machine learning đó vào ứng dụng trong thực tế. Cũng tự nhủ với mình rằng mình không phát minh ra các thuật toán đó nhưng mình sẽ ứng dụng chúng thật tốt để tạo ra thêm nhiều giá trị cho xã hội. Bài này mình sẽ giới thiệu một ứng dụng nho nhỏ của mình về Computer Vision trong việc tự động các công việc nhàm chán tại công ty. Giới thiệuĐây là dự án thứ 2 của mình tại công ty, dự án này phục vụ quá trình thu thập dữ liệu, check thông tin các khách hàng trong diện khả nghi. Do lượng thông tin khách hàng cần check khá lớn nên việc thực hiện công việc này một cách tự động sẽ mang lại rất nhiều lợi ích tiết kiệm thời gian và nguồn lực, tăng hiệu quả hoạt động.Tuy nhiên việc thực hiện tự động gặp phải khó khăn đó là việc check thông tin chỉ được hoàn tất sau khi nhập vào captcha có dạng như sau: Giải phápViệc tự động lấy dữ liệu có thể dễ dàng thực hiện với thư viện selenium của Python, khó khăn duy nhất cần phải giải quyết là phá được captcha.Với xử lý hình ảnh thì deep learning là sự lựa chọn tốt nhất rồi , open-cv cũng không thể bỏ qua được. Đối với bài toán không quá phức tạp như phá captcha, thì sử dụng các thuật toán phức tạp như yolo, ssd, mask_RCNN trở nên khá cồng kềnh. Cộng với điều kiện máy công ty không có gpu nên khi triển khai, tốc độ thực thi không đạt như mong muốn. Do vậy hướng đi sử dụng pre-trained model đã bị gạt bỏ sau một ngày thử nghiệm. Trở lại với open-cv và phân tích các đặc trưng của captcha, ta nhận thấy mỗi captcha có 5 ký tự và chỉ có 3 màu là xanh dương, vàng, đen. Do vậy có thể tách captcha theo 3 màu và cố gắp chia captcha thành 5 phần độc lập bao gồm 5 ký tự, với mỗi phần đó có thể tạo mô hình CNN để dự báo ký tự. Việc lọc màu cũng là 1 cách rất tốt để lấy mẫu. Ta chỉ cần chọn các ký tự có màu độc lập là có sẵn 1 bộ mẫu ký tự đơn, sau đó gán nhãn. Tiếp tục thực hiện train model CNN và lấy mẫu thêm theo cách trên mô hình rất nhanh sẽ được củng cố độ chính xác. Sau đây là một số kết quả dự báo từ mô hình (hình đã qua open-cv nên màu hơi khác, mình lười chỉnh lại): Và bây giờ thì việc khó khăn nhất là captcha đã được hóa giải, công việc tự động khác thì đã có selenium lo, chúng ta có thể vào lấy thông tin trên CIC dễ như ăn cháo với hiệu suất gấp mấy chục lần con người.","categories":[],"tags":[{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"},{"name":"Computer-Vision","slug":"Computer-Vision","permalink":"https://quangminh49.github.io/tags/Computer-Vision/"}]},{"title":"deploying Python applications with Nginx - Gunicorn - Python","slug":"linux-tips","date":"2019-08-25T03:17:55.000Z","updated":"2021-04-27T08:13:39.667Z","comments":true,"path":"2019/08/25/linux-tips/","link":"","permalink":"https://quangminh49.github.io/2019/08/25/linux-tips/","excerpt":"Code chán rồi thì ta qua làm Devops luôn nhé. Bài này sẽ nói về sau khi ta code Django, Flask chán chê rồi thì làm sao đưa nó lên production. Flow sẽ như trên hình, tài liệu nhiều lắm ở đây chúng ta sẽ không lam man mà tập trung vào 1 số vấn dề quan trọng trong quá trình.","text":"Code chán rồi thì ta qua làm Devops luôn nhé. Bài này sẽ nói về sau khi ta code Django, Flask chán chê rồi thì làm sao đưa nó lên production. Flow sẽ như trên hình, tài liệu nhiều lắm ở đây chúng ta sẽ không lam man mà tập trung vào 1 số vấn dề quan trọng trong quá trình. NGINXNGINX, là một nền tảng web server được phát hành năm 2004 và ngay lập tức đã được giới lập trình web đón nhận và trở nên phổ biến. wikipedia định nghĩa nó như sau: Nginx (pronounced “engine X”), stylized as NGINX, nginx or NginX, is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. The software was created by Igor Sysoev and publicly released in 2004. Nginx is free and open-source software, released under the terms of the 2-clause BSD license. A large fraction of web servers use NGINX, often as a load balancer. Gunicornwikipedia định nghĩa nó như sau: The Gunicorn “Green Unicorn” (pronounced jee-unicorn or gun-i-corn) is a Python Web Server Gateway Interface (WSGI) HTTP server. It is a pre-fork worker model, ported from Ruby’s Unicorn project. The Gunicorn server is broadly compatible with a number of web frameworks, simply implemented, light on server resources and fairly fast. Tại sau ứng dụng Python lại cần cả Gunicorn và NGINX ? step 1 : setup gunicorn.servicechạy thử gunicornflask 1gunicorn --bind 0.0.0.0:8001 wsgi django 1gunicorn --bind 0.0.0.0:8001 Wl_app_01.wsgi gunicorn.service:nếu là dự án django thì phải collectstatictrong setting.py : 1STATIC_ROOT = os.path.join(PROJECT_ROOT, &#x27;staticfiles&#x27;) chạy lệnh để collectstatic qua /staticfiles/: 1python manage.py collectstatic 12cd /etc/systemd/systemtouch my_dạngo_app.service Content: 12345678910111213[Unit]Description=Gunicorn instance to serve AppAfter=network.target[Service]User=www-userGroup=www-groupWorkingDirectory=/opt/my_dạngo_appEnvironment=&quot;PATH=/opt/my_dạngo_app/venv/bin&quot;ExecStart=/opt/wl-app-01/venv/bin/gunicorn --workers 3 --bind 127.0.0.1:5003 -m 007 my_django_app.wsgi:application[Install]WantedBy=multi-user.target start service: 1234systemctl enable my_dạngo_app # set enable, service sẽ tự khởi động cùng hệ thống (mất điện, sập server)systemctl start my_dạngo_appsystemctl status my_dạngo_app step 2 : config nginx12cd /etc/nginx/conf.d/touch my_dạngo_app.conf Content: 123456789101112131415161718192021222324server &#123; listen 80; server_name _; # server_name = _ sẽ chạy trên 0.0.0.0 , còn đúng ra sẽ bỏ domain vào vd: jav.vn.com; access_log /var/log/nginx/wl-app-access.log; error_log /var/log/nginx/wl-app-error.log; location / &#123; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Host $host; proxy_pass http://127.0.0.1:5003; #port mở bằng service gunicorn &#125; location /static/&#123; alias /opt/my_dạngo_app/my_dạngo_app/staticfiles/; #nhớ set dir vừa collectstatic &#125;&#125; 123sudo systemctl enable nginx.servicesudo systemctl start nginx.servicesudo systemctl status nginx.service Lỗi bất cậplỗi page admin: không đăng nhập được user admin khi sử dụng MongoDb backend:Vẫn tạo superuser “root” như bình thường nhưng vẫn lỗi có thể là do trong collection auth_user thiếu trường id, cần thêm vào ở dạng NumberIntquery thử collection db.auth_user, xem có trường ‘id’ chưa, nếu chưa có thì thêm vào: 1db.auth_user.updateOne(&#123;&quot;username&quot; : &quot;root&quot;&#125;, &#123;$set:&#123;&#x27;id&#x27;: NumberInt(1)&#125;&#125;)","categories":[],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://quangminh49.github.io/tags/DevOps/"}]}],"categories":[],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://quangminh49.github.io/tags/DevOps/"},{"name":"CI/CD","slug":"CI-CD","permalink":"https://quangminh49.github.io/tags/CI-CD/"},{"name":"projects","slug":"projects","permalink":"https://quangminh49.github.io/tags/projects/"},{"name":"Full-Stack","slug":"Full-Stack","permalink":"https://quangminh49.github.io/tags/Full-Stack/"},{"name":"Data-Science","slug":"Data-Science","permalink":"https://quangminh49.github.io/tags/Data-Science/"},{"name":"NLP","slug":"NLP","permalink":"https://quangminh49.github.io/tags/NLP/"},{"name":"HCM-housing","slug":"HCM-housing","permalink":"https://quangminh49.github.io/tags/HCM-housing/"},{"name":"Statistic","slug":"Statistic","permalink":"https://quangminh49.github.io/tags/Statistic/"},{"name":"Credit-Scorecard","slug":"Credit-Scorecard","permalink":"https://quangminh49.github.io/tags/Credit-Scorecard/"},{"name":"Google-Ecosystem","slug":"Google-Ecosystem","permalink":"https://quangminh49.github.io/tags/Google-Ecosystem/"},{"name":"Computer-Vision","slug":"Computer-Vision","permalink":"https://quangminh49.github.io/tags/Computer-Vision/"}]}